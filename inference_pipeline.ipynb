{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a19676",
   "metadata": {
    "_cell_guid": "fd1b354a-917c-4776-9773-ee2d22996d25",
    "_uuid": "6b9078b5-d216-4709-afd2-27615a5a3a57",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-11T13:34:04.799731Z",
     "iopub.status.busy": "2024-12-11T13:34:04.799478Z",
     "iopub.status.idle": "2024-12-11T13:37:45.962137Z",
     "shell.execute_reply": "2024-12-11T13:37:45.961054Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 221.170926,
     "end_time": "2024-12-11T13:37:45.963970",
     "exception": false,
     "start_time": "2024-12-11T13:34:04.793044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "CPU times: user 2.18 s, sys: 570 ms, total: 2.75 s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b0b9d5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:37:45.975614Z",
     "iopub.status.busy": "2024-12-11T13:37:45.975246Z",
     "iopub.status.idle": "2024-12-11T13:37:54.333990Z",
     "shell.execute_reply": "2024-12-11T13:37:54.333048Z"
    },
    "papermill": {
     "duration": 8.366926,
     "end_time": "2024-12-11T13:37:54.336081",
     "exception": false,
     "start_time": "2024-12-11T13:37:45.969155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfd539a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:37:54.348884Z",
     "iopub.status.busy": "2024-12-11T13:37:54.348156Z",
     "iopub.status.idle": "2024-12-11T13:38:02.468610Z",
     "shell.execute_reply": "2024-12-11T13:38:02.467373Z"
    },
    "papermill": {
     "duration": 8.129463,
     "end_time": "2024-12-11T13:38:02.470699",
     "exception": false,
     "start_time": "2024-12-11T13:37:54.341236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pylatexenc -q -U --no-index --find-links /kaggle/input/pylatexenc-wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbee01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:02.482127Z",
     "iopub.status.busy": "2024-12-11T13:38:02.481822Z",
     "iopub.status.idle": "2024-12-11T13:38:07.233410Z",
     "shell.execute_reply": "2024-12-11T13:38:07.231858Z"
    },
    "papermill": {
     "duration": 4.760268,
     "end_time": "2024-12-11T13:38:07.236098",
     "exception": false,
     "start_time": "2024-12-11T13:38:02.475830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 13:38:06,076\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os, math, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import transformers\n",
    "import vllm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9365996f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:07.252602Z",
     "iopub.status.busy": "2024-12-11T13:38:07.252089Z",
     "iopub.status.idle": "2024-12-11T13:38:07.256863Z",
     "shell.execute_reply": "2024-12-11T13:38:07.255944Z"
    },
    "papermill": {
     "duration": 0.012633,
     "end_time": "2024-12-11T13:38:07.258592",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.245959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python3.10/site-packages/ray/thirdparty_files', '/kaggle/lib/kagglegym', '/kaggle/lib', '/kaggle/input/eedi-mining-misconceptions-in-mathematics', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/root/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages', '/root/src/BigQuery_Helper']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2295871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:07.269959Z",
     "iopub.status.busy": "2024-12-11T13:38:07.269642Z",
     "iopub.status.idle": "2024-12-11T13:38:07.274843Z",
     "shell.execute_reply": "2024-12-11T13:38:07.273797Z"
    },
    "papermill": {
     "duration": 0.01291,
     "end_time": "2024-12-11T13:38:07.276450",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.263540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "print(IS_SUBMISSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95544cd0",
   "metadata": {
    "papermill": {
     "duration": 0.004908,
     "end_time": "2024-12-11T13:38:07.286272",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.281364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metrics\n",
    "The metrics that we will use for this problem: MAP@25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1007c231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:07.297570Z",
     "iopub.status.busy": "2024-12-11T13:38:07.297216Z",
     "iopub.status.idle": "2024-12-11T13:38:07.306900Z",
     "shell.execute_reply": "2024-12-11T13:38:07.306117Z"
    },
    "papermill": {
     "duration": 0.017834,
     "end_time": "2024-12-11T13:38:07.308965",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.291131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing eedi_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eedi_metrics.py\n",
    "\n",
    "# Credit: https://www.kaggle.com/code/abdullahmeda/eedi-map-k-metric\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def apk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        # first condition checks whether it is valid prediction\n",
    "        # second condition checks if prediction is not repeated\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def recall_at_k(actual, predicted, k=25):\n",
    "    \"\"\"\n",
    "    Computes the recall at k, handling cases where the inputs might be numpy arrays or lists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : int/float, single element\n",
    "    predicted : list\n",
    "                A list of predicted elements (order matters up to k)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements to consider\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "            The recall at k\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    # Convert numpy arrays or lists to tuples if necessary\n",
    "    actual = int(actual[0])\n",
    "    predicted = {int(item) if isinstance(item, (np.ndarray, list)) else item for item in predicted}\n",
    "    # print(\"Actual: \", actual)\n",
    "    # print(\"predicted: \", predicted)\n",
    "    # print(\"Actual type: \", type(actual))\n",
    "    # print(\"Predicted type: \", type(predicted))\n",
    "    if actual in predicted:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def averaged_recall_at_k(actual, predicted, k=25):\n",
    "    return np.mean([recall_at_k(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def dcg_at_k(relevances, k):\n",
    "    \"\"\"Compute the Discounted Cumulative Gain at rank k\"\"\"\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size:\n",
    "        return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    \"\"\"Compute the Normalized Discounted Cumulative Gain at rank k\"\"\"\n",
    "    if k > len(predicted):\n",
    "        k = len(predicted)\n",
    "\n",
    "    # Actual relevance: 1 if the item is the ground truth, 0 otherwise\n",
    "    actual_relevance = [1 if p == actual else 0 for p in predicted]\n",
    "\n",
    "    # Compute DCG@k\n",
    "    dcg_score = dcg_at_k(actual_relevance, k)\n",
    "\n",
    "    # Compute IDCG@k (best possible DCG@k)\n",
    "    idcg_score = dcg_at_k([1], k)  # Best case: the relevant item is at the top of the list\n",
    "    if idcg_score == 0:\n",
    "        return 0.0\n",
    "    return dcg_score / idcg_score\n",
    "def averaged_ndcg_at_k(actual, predicted, k=25):\n",
    "    return np.mean([ndcg_at_k(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130f485",
   "metadata": {
    "papermill": {
     "duration": 0.008687,
     "end_time": "2024-12-11T13:38:07.326447",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.317760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM reasoning\n",
    "Use Qwen2.5 14B Instruct AWQ to generate Reasoning HyDE if necessary. Possible choices:\n",
    "1. Generate HyDE with LLM misconception zero-shot reasoning\n",
    "2. Generate HyDE with LLM misconception, with self consistency\n",
    "3. No reasoning, just reformat the question into a query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefd3ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:07.345579Z",
     "iopub.status.busy": "2024-12-11T13:38:07.345314Z",
     "iopub.status.idle": "2024-12-11T13:38:07.357171Z",
     "shell.execute_reply": "2024-12-11T13:38:07.356232Z"
    },
    "papermill": {
     "duration": 0.023538,
     "end_time": "2024-12-11T13:38:07.358787",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.335249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import vllm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import multiprocessing\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#clear cache\n",
    "torch.cuda.empty_cache()\n",
    "use_online_synthetic_data=False\n",
    "activate_reasoning = True\n",
    "SELF_CONSISTENCY = False #only be used if this is a reasoning problem\n",
    "SELF_CONSISTENCY_AMT=5 #only be used if SELF_CONSISTENCY_AMT is True\n",
    "#LLM_MODEL_PATH=\"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "LLM_MODEL_PATH=\"/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ\"\n",
    "TRAIN_MODE = False\n",
    "\n",
    "\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "if use_online_synthetic_data:\n",
    "    df_train = pd.read_csv(\"/flash2/aml/wangwd24/eedi-mining-misconceptions-in-mathematics/eedi_synthetic.csv\").fillna(-1)\n",
    "else:\n",
    "    df_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1)\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_PATH)\n",
    "PROMPT_ORIGINAL  = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: You are a genius mathematician. Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_QUERY_NO_REASON = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_template(row, tokenizer, targetCol, online_synthetic_data=False):\n",
    "    if not online_synthetic_data:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": PROMPT_ORIGINAL.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n",
    "                    CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"])\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": PROMPT_ORIGINAL.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"AnswerText\"],\n",
    "                    CorrectAnswer=row[f\"CorrectAnswerText\"])\n",
    "            }\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "def apply_template_no_reason(row, targetCol, online_synthetic_data=False):\n",
    "    if not online_synthetic_data:\n",
    "        messages = [\n",
    "            PROMPT_QUERY_NO_REASON.format(\n",
    "                ConstructName=row[\"ConstructName\"],\n",
    "                SubjectName=row[\"SubjectName\"],\n",
    "                Question=row[\"QuestionText\"],\n",
    "                IncorrectAnswer=row[f\"Answer{targetCol}Text\"],\n",
    "                CorrectAnswer=row[f\"Answer{row.CorrectAnswer}Text\"])\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": PROMPT_ORIGINAL.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"AnswerText\"],\n",
    "                    CorrectAnswer=row[f\"CorrectAnswerText\"])\n",
    "            }\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "print(\"Executing: \")\n",
    "df = {}\n",
    "question_level_df_data = {} #make a dataframe that maps to each question level\n",
    "#initialize required column\n",
    "question_level_df_data[\"QuestionId_Answer\"] = []\n",
    "question_level_df_data[\"ConstructName\"] = []\n",
    "question_level_df_data[\"SubjectName\"] = []\n",
    "question_level_df_data[\"QuestionText\"] = []\n",
    "question_level_df_data[\"incorrect_answer\"] = []\n",
    "question_level_df_data[\"correct_answer\"] = []\n",
    "if TRAIN_MODE:\n",
    "    print(\"Is not submission\")\n",
    "    df_label = {}\n",
    "    for idx, row in df_train.iterrows():\n",
    "        if not use_online_synthetic_data:\n",
    "            for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                if (row.CorrectAnswer!=option) & (row[f\"Misconception{option}Id\"]!=-1):\n",
    "                    #populate question level dataframe\n",
    "                    question_level_df_data[\"QuestionId_Answer\"].append(f\"{row.QuestionId}_{option}\")\n",
    "                    question_level_df_data[\"ConstructName\"].append(row[\"ConstructName\"])\n",
    "                    question_level_df_data[\"SubjectName\"].append(row[\"SubjectName\"])\n",
    "                    question_level_df_data[\"QuestionText\"].append(row[\"QuestionText\"])\n",
    "                    question_level_df_data[\"incorrect_answer\"].append(row[f\"Answer{option}Text\"])\n",
    "                    question_level_df_data[\"correct_answer\"].append(row[f\"Answer{row.CorrectAnswer}Text\"])\n",
    "\n",
    "                    if activate_reasoning:\n",
    "                        df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n",
    "                    else:\n",
    "                        df[f\"{row.QuestionId}_{option}\"] = apply_template_no_reason(row, option)\n",
    "                    df_label[f\"{row.QuestionId}_{option}\"] = [row[f\"Misconception{option}Id\"]]\n",
    "        else:\n",
    "            #kaggle online synthetic data\n",
    "            if activate_reasoning:\n",
    "                df[f\"Synthetic_{idx}\"] = apply_template(row, tokenizer, \"\", online_synthetic_data=True)\n",
    "            else:\n",
    "                df[f\"Synthetic_{idx}\"] = apply_template_no_reason(row, \"\", online_synthetic_data=True)\n",
    "            df_label[f\"Synthetic_{idx}\"] = [row[\"MisconceptionID\"]]\n",
    "    df_label = pd.DataFrame([df_label]).T.reset_index()\n",
    "    df_label.columns = [\"QuestionId_Answer\", \"MisconceptionId\"]\n",
    "    df_label.to_parquet(\"/kaggle/working/label.parquet\", index=False)\n",
    "else:\n",
    "    print(\"Entering test mode\")\n",
    "    for idx, row in df_test.iterrows():\n",
    "        print(\"Row question id: \", row.QuestionId)\n",
    "        for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if row.CorrectAnswer!=option:\n",
    "                #populate question level dataframe\n",
    "                question_level_df_data[\"QuestionId_Answer\"].append(f\"{row.QuestionId}_{option}\")\n",
    "                question_level_df_data[\"ConstructName\"].append(row[\"ConstructName\"])\n",
    "                question_level_df_data[\"SubjectName\"].append(row[\"SubjectName\"])\n",
    "                question_level_df_data[\"QuestionText\"].append(row[\"QuestionText\"])\n",
    "                question_level_df_data[\"incorrect_answer\"].append(row[f\"Answer{option}Text\"])\n",
    "                question_level_df_data[\"correct_answer\"].append(row[f\"Answer{row.CorrectAnswer}Text\"])\n",
    "\n",
    "                if activate_reasoning:\n",
    "                    df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer, option)\n",
    "                else:\n",
    "                    df[f\"{row.QuestionId}_{option}\"] = apply_template(row, tokenizer)\n",
    "\n",
    "df = pd.DataFrame([df]).T.reset_index()\n",
    "df.columns = [\"QuestionId_Answer\", \"text\"]\n",
    "df.to_parquet(\"/kaggle/working/temporary.parquet\", index=False)\n",
    "\n",
    "# =========================== Beginning input generation for embedding model ===========================\n",
    "if TRAIN_MODE:\n",
    "    if use_online_synthetic_data:\n",
    "        df_gt_addr = \"/flash2/aml/wangwd24/eedi-mining-misconceptions-in-mathematics/eedi_synthetic.csv\"\n",
    "    else:\n",
    "        df_gt_addr = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\"\n",
    "        \n",
    "else:\n",
    "    df_gt_addr = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\"\n",
    "\n",
    "df = pd.read_parquet(\"/kaggle/working/temporary.parquet\")\n",
    "df_train = pd.read_csv(df_gt_addr).fillna(-1)\n",
    "print(\"df_train head: \")\n",
    "print(df_train.head())\n",
    "\n",
    "PROMPT_SELF_CONSISTENCY = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: You are a genius mathematician. Identify the misconception behind Incorrect Answer. You have multiple reasoning paths behind misconceptions and use self-consitency to choose the accurate one. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\n",
    "\n",
    "{ReasoningPaths}\n",
    "\"\"\"\n",
    "\n",
    "def apply_template_self_consistency(row_from_df, tokenizer, reason_path_responses, targetCol, online_synthetic_data=False):\n",
    "    reasoning_text = \"\"\n",
    "    reasoning_length = len(reason_path_responses)\n",
    "    for idx in range(reasoning_length):\n",
    "        letter_idx = chr(ord('A') + idx)\n",
    "        reasoning_text += f\"\"\"Reasoning {letter_idx}\n",
    "        {reason_path_responses[idx]}\n",
    "        \n",
    "        \"\"\"\n",
    "    if not online_synthetic_data:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": PROMPT_SELF_CONSISTENCY.format(\n",
    "                    ConstructName=row_from_df[\"ConstructName\"],\n",
    "                    SubjectName=row_from_df[\"SubjectName\"],\n",
    "                    Question=row_from_df[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row_from_df[f\"Answer{targetCol}Text\"],\n",
    "                    CorrectAnswer=row_from_df[f\"Answer{row_from_df.CorrectAnswer}Text\"],\n",
    "                    ReasoningPaths=reasoning_text),\n",
    "            }\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": PROMPT_SELF_CONSISTENCY.format(\n",
    "                    ConstructName=row_from_df[\"ConstructName\"],\n",
    "                    SubjectName=row_from_df[\"SubjectName\"],\n",
    "                    Question=row_from_df[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row_from_df[f\"AnswerText\"],\n",
    "                    CorrectAnswer=row_from_df[f\"CorrectAnswerText\"],\n",
    "                    ReasoningPaths=reasoning_text),\n",
    "            }\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    LLM_MODEL_PATH,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,#8192\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "df_new = {}\n",
    "\n",
    "if activate_reasoning: \n",
    "    if SELF_CONSISTENCY:\n",
    "        print(\"Self Consistency running!\")\n",
    "        responses_list = []\n",
    "        for i in range(SELF_CONSISTENCY_AMT):\n",
    "            responses = llm.generate(\n",
    "                df[\"text\"].values,\n",
    "                vllm.SamplingParams(\n",
    "                    n=1,  # Number of output sequences to return for each prompt.\n",
    "                    top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "                    temperature=0.1,  # randomness of the sampling\n",
    "                    seed=777, # Seed for reprodicibility\n",
    "                    skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "                    max_tokens=2048,  # Maximum number of tokens to generate per output sequence.\n",
    "                ),\n",
    "                use_tqdm = True\n",
    "            )\n",
    "            responses = [x.outputs[0].text for x in responses]\n",
    "            responses_list.append(responses)\n",
    "        responses_list = list(zip(*responses_list))\n",
    "        counter = 0\n",
    "        final_text = []\n",
    "        for idx, row_from_df in df_train.iterrows():\n",
    "            if not use_online_synthetic_data:\n",
    "                for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                    if TRAIN_MODE:\n",
    "                        boolForSelfConsistency = (row_from_df.CorrectAnswer!=option) & (row_from_df[f\"Misconception{option}Id\"]!=-1)\n",
    "                    else:\n",
    "                        boolForSelfConsistency = (row_from_df.CorrectAnswer!=option)\n",
    "                    if boolForSelfConsistency:\n",
    "                        temp_text = apply_template_self_consistency(row_from_df, tokenizer, responses_list[counter], option)\n",
    "                        counter += 1\n",
    "                        final_text.append(temp_text)\n",
    "            else:\n",
    "                temp_text = apply_template_self_consistency(row_from_df, tokenizer, responses_list[counter], \"\", online_synthetic_data=True)\n",
    "                counter += 1\n",
    "                final_text.append(temp_text)\n",
    "\n",
    "\n",
    "        print(\"Final text: \", final_text)\n",
    "\n",
    "        responses = llm.generate(\n",
    "            final_text,\n",
    "            vllm.SamplingParams(\n",
    "                n=1,  # Number of output sequences to return for each prompt.\n",
    "                top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "                temperature=0,  # randomness of the sampling\n",
    "                seed=777, # Seed for reprodicibility\n",
    "                skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "                max_tokens=2048,  # Maximum number of tokens to generate per output sequence.\n",
    "            ),\n",
    "            use_tqdm = True\n",
    "        )\n",
    "    else:\n",
    "        print(\"Self Consistency not running!\")\n",
    "        responses = llm.generate(\n",
    "            df[\"text\"].values,\n",
    "            vllm.SamplingParams(\n",
    "                n=1,  # Number of output sequences to return for each prompt.\n",
    "                top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "                temperature=0,  # randomness of the sampling\n",
    "                seed=777, # Seed for reprodicibility\n",
    "                skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "                max_tokens=2048,  # Maximum number of tokens to generate per output sequence.\n",
    "            ),\n",
    "            use_tqdm = True\n",
    "        )\n",
    "    responses = [x.outputs[0].text for x in responses]\n",
    "    df[\"fullLLMText\"] = responses\n",
    "\n",
    "    def extract_response(text):\n",
    "        return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n",
    "\n",
    "    responses = [extract_response(x) for x in responses]\n",
    "    #responses_new_prompt = [extract_response(x) for x in responses_new_prompt]\n",
    "    df[\"llmMisconception\"] = responses\n",
    "    #df_new_prompt[\"llmMisconception\"] = responses\n",
    "    if SELF_CONSISTENCY:\n",
    "        if use_online_synthetic_data:\n",
    "            df.to_parquet(f\"/kaggle/working/submission.parquet\", index=False)\n",
    "        else:\n",
    "            df.to_parquet(f\"/kaggle/working/submission.parquet\", index=False)\n",
    "    else:\n",
    "        if use_online_synthetic_data:\n",
    "            df.to_parquet(\"/kaggle/working/submission.parquet\", index=False)\n",
    "        else:\n",
    "            df.to_parquet(f\"/kaggle/working/submission.parquet\", index=False)\n",
    "            print(\"File saved\")\n",
    "else:\n",
    "    print(\"No reasoning executed!\")\n",
    "    df[\"fullLLMText\"] = df[\"text\"]\n",
    "    df[\"llmMisconception\"] = df[\"text\"]\n",
    "    if use_online_synthetic_data:\n",
    "        df.to_parquet(\"/kaggle/working/submission.parquet\", index=False)\n",
    "    else:\n",
    "        df.to_parquet(\"/kaggle/working/submission.parquet\", index=False)\n",
    "\n",
    "#provide question level data and label if needed\n",
    "question_level_df = pd.DataFrame(question_level_df_data)\n",
    "question_level_df.to_parquet(\"/kaggle/working/question_level_df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1433959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:07.370074Z",
     "iopub.status.busy": "2024-12-11T13:38:07.369749Z",
     "iopub.status.idle": "2024-12-11T13:39:40.469875Z",
     "shell.execute_reply": "2024-12-11T13:39:40.468716Z"
    },
    "papermill": {
     "duration": 93.108132,
     "end_time": "2024-12-11T13:39:40.471984",
     "exception": false,
     "start_time": "2024-12-11T13:38:07.363852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: \r\n",
      "Entering test mode\r\n",
      "Row question id:  1869\r\n",
      "Row question id:  1870\r\n",
      "Row question id:  1871\r\n",
      "df_train head: \r\n",
      "   QuestionId  ConstructId  ...            AnswerCText             AnswerDText\r\n",
      "0        1869          856  ...  \\( 3 \\times(2+4-5) \\)  Does not need brackets\r\n",
      "1        1870         1612  ...              \\( m-1 \\)       Does not simplify\r\n",
      "2        1871         2774  ...     Both Tom and Katie      Neither is correct\r\n",
      "\r\n",
      "[3 rows x 11 columns]\r\n",
      "WARNING 12-11 13:38:12 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 13:38:12 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-11 13:38:12 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ', speculative_config=None, tokenizer='/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-11 13:38:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-11 13:38:12 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:12 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 13:38:12 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:12 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "INFO 12-11 13:38:15 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:15 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:15 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 13:38:15 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 13:38:15 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 13:38:22 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:22 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 13:38:22 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7b4d604a65f0>, local_subscribe_port=46777, local_sync_port=40963, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-11 13:38:22 model_runner.py:680] Starting to load model /kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:22 model_runner.py:680] Starting to load model /kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:23 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:38:23 selector.py:54] Using XFormers backend.\r\n",
      "INFO 12-11 13:38:23 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 13:38:23 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:09<00:19,  9.90s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:30<00:16, 16.13s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:53<00:00, 19.13s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:53<00:00, 17.70s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=103)\u001b[0;0m INFO 12-11 13:39:16 model_runner.py:692] Loading model weights took 4.6719 GB\r\n",
      "INFO 12-11 13:39:16 model_runner.py:692] Loading model weights took 4.6719 GB\r\n",
      "INFO 12-11 13:39:20 distributed_gpu_executor.py:56] # GPU blocks: 4150, # CPU blocks: 2730\r\n",
      "Self Consistency not running!\r\n",
      "Processed prompts: 100%|█| 9/9 [00:08<00:00,  1.01it/s, est. speed input: 222.70\r\n",
      "File saved\r\n",
      "[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eca35cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:39:40.487329Z",
     "iopub.status.busy": "2024-12-11T13:39:40.486995Z",
     "iopub.status.idle": "2024-12-11T13:39:40.494087Z",
     "shell.execute_reply": "2024-12-11T13:39:40.493249Z"
    },
    "papermill": {
     "duration": 0.016944,
     "end_time": "2024-12-11T13:39:40.495676",
     "exception": false,
     "start_time": "2024-12-11T13:39:40.478732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llm_output = pd.read_parquet(\"/kaggle/working/submission.parquet\")\\nprint(llm_output.head())\\nif not IS_SUBMISSION:\\n    label = pd.read_parquet(\"/kaggle/working/label.parquet\")\\n    print(\"Label: \")\\n    print(label.head())\\n    print(\"=====\")\\n    label[\"MisconceptionReasoning\"] = llm_output[\"llmMisconception\"]\\n\\n    print(label[\"MisconceptionReasoning\"][0])\\n    label.to_parquet(\"temp.parquet\", index=False)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"llm_output = pd.read_parquet(\"/kaggle/working/submission.parquet\")\n",
    "print(llm_output.head())\n",
    "if not IS_SUBMISSION:\n",
    "    label = pd.read_parquet(\"/kaggle/working/label.parquet\")\n",
    "    print(\"Label: \")\n",
    "    print(label.head())\n",
    "    print(\"=====\")\n",
    "    label[\"MisconceptionReasoning\"] = llm_output[\"llmMisconception\"]\n",
    "\n",
    "    print(label[\"MisconceptionReasoning\"][0])\n",
    "    label.to_parquet(\"temp.parquet\", index=False)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423fb1c",
   "metadata": {
    "papermill": {
     "duration": 0.006486,
     "end_time": "2024-12-11T13:39:40.508721",
     "exception": false,
     "start_time": "2024-12-11T13:39:40.502235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [](http://)Embedding Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489a86aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:39:40.522655Z",
     "iopub.status.busy": "2024-12-11T13:39:40.522409Z",
     "iopub.status.idle": "2024-12-11T13:39:40.540834Z",
     "shell.execute_reply": "2024-12-11T13:39:40.539815Z"
    },
    "papermill": {
     "duration": 0.027377,
     "end_time": "2024-12-11T13:39:40.542491",
     "exception": false,
     "start_time": "2024-12-11T13:39:40.515114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   QuestionId  ConstructId                                      ConstructName  \\\n",
      "0        1869          856  Use the order of operations to carry out calcu...   \n",
      "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
      "2        1871         2774            Calculate the range from a list of data   \n",
      "\n",
      "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
      "0         33                                             BIDMAS             A   \n",
      "1       1077                    Simplifying Algebraic Fractions             D   \n",
      "2        339  Range and Interquartile Range from a List of Data             B   \n",
      "\n",
      "                                        QuestionText            AnswerAText  \\\n",
      "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
      "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
      "2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
      "\n",
      "              AnswerBText            AnswerCText             AnswerDText  \n",
      "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets  \n",
      "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify  \n",
      "2             Only\\nKatie     Both Tom and Katie      Neither is correct  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1198e7",
   "metadata": {},
   "source": [
    "# Run finetuned embedding model search\n",
    "Use finetuned SFR Mistral 2R to search for the nearest misconception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c9349f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:39:40.557231Z",
     "iopub.status.busy": "2024-12-11T13:39:40.557001Z",
     "iopub.status.idle": "2024-12-11T13:39:40.567161Z",
     "shell.execute_reply": "2024-12-11T13:39:40.566312Z"
    },
    "papermill": {
     "duration": 0.019549,
     "end_time": "2024-12-11T13:39:40.568815",
     "exception": false,
     "start_time": "2024-12-11T13:39:40.549266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing embedding_search_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embedding_search_setup.py\n",
    "# Import Libraries\n",
    "##########################################################################\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# Configurations\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#clear cache\n",
    "torch.cuda.empty_cache()\n",
    "CONFIG = {\n",
    "    \"retrieve_num\": 25,\n",
    "    \"data_path\": \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\",\n",
    "    \"model_name\": \"/kaggle/input/sfr-embedding-mistral/SFR-Embedding-2_R\",\n",
    "    \"output_path\": \"/kaggle/working/submission.parquet\",\n",
    "    \"target_column\": \"AllTextWithLlmMisconceptionCleaned\",\n",
    "    \"checkpoint_path\": \"/kaggle/input/sfr-2-r-zeroshotselfconsistency-higherloradropout\",\n",
    "    \"misconception_path\": \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\",\n",
    "    \"llm_answer_path\": \"/kaggle/working/submission.parquet\",# have to change it\n",
    "}\n",
    "\n",
    "# Data Preparation\n",
    "##########################################################################\n",
    "def load_data():\n",
    "    train = pl.read_csv(CONFIG['data_path'])\n",
    "    misconception_mapping = pl.read_csv(CONFIG['misconception_path'])\n",
    "    llm_answers = pl.read_parquet(CONFIG[\"llm_answer_path\"])\n",
    "    return train, misconception_mapping, llm_answers\n",
    "\n",
    "def convert_to_long(df: pl.DataFrame,\n",
    "                    common_cols: list =[\"QuestionId\", \"ConstructName\", \"SubjectName\", \"QuestionText\", \"CorrectAnswer\"],\n",
    "                   ):\n",
    "    df_long = (\n",
    "        df\n",
    "        .select(\n",
    "            pl.col(common_cols + [f\"Answer{alpha}Text\" for alpha in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "        )\n",
    "        .unpivot(\n",
    "            index=common_cols,\n",
    "            variable_name=\"AnswerType\",\n",
    "            value_name=\"AnswerText\",\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.concat_str(\n",
    "                [\n",
    "                    pl.format(\"Construct Name:\\n{}\\n\\n\", pl.col(\"ConstructName\")),\n",
    "                    pl.format(\"Subject Name:\\n{}\\n\\n\", pl.col(\"SubjectName\")),\n",
    "                    pl.format(\"Question Text:\\n{}\\n\\n\", pl.col(\"QuestionText\")),\n",
    "                    pl.format(\"Answer Text:\\n{}\\n\\n\", pl.col(\"AnswerText\")),\n",
    "                ],\n",
    "            separator=\"\"\n",
    "            ).alias(\"AllText\"),\n",
    "            pl.col(\"AnswerType\").str.extract(r\"Answer([A-D])Text$\").alias(\"AnswerAlphabet\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.concat_str(\n",
    "                [pl.col(\"QuestionId\"), pl.col(\"AnswerAlphabet\")], separator=\"_\"\n",
    "            ).alias(\"QuestionId_Answer\"),\n",
    "        )\n",
    "        .sort(\"QuestionId_Answer\")\n",
    "    )\n",
    "    print(\"result in function\")\n",
    "    print(df_long.head())\n",
    "    print(\"--------\")\n",
    "    return df_long\n",
    "\n",
    "def add_llm_misconceptions(df_long: pl.DataFrame, llm_answers: pl.DataFrame) -> pl.DataFrame:\n",
    "    df_merged = df_long.join(llm_answers[['QuestionId_Answer', 'llmMisconception']], on='QuestionId_Answer', how='left' )\n",
    "    df_merged = df_merged.with_columns(pl.concat_str([pl.col('AllText'),pl.format(\"Misconception:\\n{}\", pl.col(\"llmMisconception\"))]).alias(CONFIG[\"target_column\"]))\n",
    "    return df_merged\n",
    "\n",
    "def preprocess_latex_column(df, column):\n",
    "    def clean_latex(row):\n",
    "        clean_text = LatexNodes2Text().latex_to_text(row[column])\n",
    "        return re.sub(r\":\\n+\\s*\", \":\\n\", clean_text)\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.struct(pl.all()).map_elements(clean_latex, return_dtype=pl.String).alias(column)\n",
    "    ])\n",
    "\n",
    "train, misconception_mapping, llm_answers = load_data()\n",
    "train_long = convert_to_long(train)\n",
    "train_long = add_llm_misconceptions(train_long, llm_answers)\n",
    "train_long = preprocess_latex_column(train_long, CONFIG[\"target_column\"])\n",
    "train_long = train_long.drop_nulls(CONFIG[\"target_column\"])\n",
    "\n",
    "# Load model\n",
    "##########################################################################\n",
    "from transformers import AutoModel\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load a model and distribute it across GPUs 1 and 2.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the PEFT model checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        model: The loaded model distributed across GPUs.\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "    \"\"\"\n",
    "    # Load the model with the device map\n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"balanced\"\n",
    "    )\n",
    "\n",
    "    # Add PEFT layers\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        is_trainable=False,\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "    print(f\"PEFT model loaded from '{checkpoint_path}' across GPUs 1 and 2.\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(CONFIG[\"checkpoint_path\"])\n",
    "model.eval()\n",
    "\n",
    "def prepare_queries_and_corpus(val_ds: Dataset, misconception_mapping: pd.DataFrame, target_column: str):\n",
    "    \"\"\"\n",
    "    Prepare queries and corpus dictionaries for evaluator and inference.\n",
    "\n",
    "    Args:\n",
    "        val_ds (Dataset): Validation dataset.\n",
    "        misconception_mapping (pd.DataFrame): Mapping of misconception IDs to names.\n",
    "        target_column (str): Column name containing query text.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (queries, corpus) dictionaries.\n",
    "    \"\"\"\n",
    "    # Prepare corpus from misconception mapping\n",
    "    corpus = dict(\n",
    "        zip(misconception_mapping[\"MisconceptionId\"], misconception_mapping[\"MisconceptionName\"])\n",
    "    )\n",
    "    \n",
    "    # Prepare queries from validation dataset\n",
    "    queries = dict(\n",
    "        zip(val_ds[\"QuestionId_Answer\"], val_ds[target_column])\n",
    "    )\n",
    "\n",
    "    return queries, corpus\n",
    "\n",
    "queries, corpus = prepare_queries_and_corpus(\n",
    "    val_ds=train_long,\n",
    "    misconception_mapping=misconception_mapping,\n",
    "    target_column=CONFIG[\"target_column\"]\n",
    ")\n",
    "print(f\"Queries: {len(queries)}\")\n",
    "print(f\"Corpus: {len(corpus)}\")\n",
    "\n",
    "# Sentece encoding\n",
    "##########################################################################\n",
    "# def encode_sentences(sentences, model, tokenizer, batch_size=64):\n",
    "#     \"\"\"\n",
    "#     Encodes a list of sentences in batches using a model split across multiple GPUs.\n",
    "\n",
    "#     Args:\n",
    "#         sentences (list of str): The sentences to encode.\n",
    "#         model: The Hugging Face transformer model distributed across GPUs.\n",
    "#         tokenizer: The tokenizer corresponding to the model.\n",
    "#         batch_size (int): The number of sentences to process per batch.\n",
    "\n",
    "#     Returns:\n",
    "#         torch.Tensor: The concatenated encoded sentence embeddings.\n",
    "#     \"\"\"\n",
    "#     all_embeddings = []\n",
    "#     model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "#     # Get the device of the first layer from the device map\n",
    "#     first_layer_device = next(iter(model.hf_device_map.values()))  # Get the device of the first layer\n",
    "\n",
    "#     # Process sentences in batches\n",
    "#     for i in range(0, len(sentences), batch_size):\n",
    "#         batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "#         # Tokenize batch and send it to the first layer's device\n",
    "#         inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\").to(first_layer_device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "\n",
    "#         # Extract token embeddings\n",
    "#         token_embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "#         attention_mask = inputs.attention_mask  # Shape: (batch_size, seq_len)\n",
    "\n",
    "#         # Compute sentence embeddings by averaging token embeddings (masked)\n",
    "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "#         sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "#         sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "#         sentence_embeddings = sum_embeddings / sum_mask  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "#         # Collect embeddings on CPU\n",
    "#         all_embeddings.append(sentence_embeddings)\n",
    "#         print(f\"Encoded batch {i}\")\n",
    "#     # Concatenate all embeddings\n",
    "#     return torch.cat(all_embeddings, dim=0).cpu()\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def encode_sentences(sentences, model, tokenizer, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encodes a list of sentences in batches using a model split across multiple GPUs.\n",
    "\n",
    "    Args:\n",
    "        sentences (list of str): The sentences to encode.\n",
    "        model: The Hugging Face transformer model distributed across GPUs.\n",
    "        tokenizer: The tokenizer corresponding to the model.\n",
    "        batch_size (int): The number of sentences to process per batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The concatenated encoded sentence embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Get the device of the first layer from the device map\n",
    "    first_layer_device = next(iter(model.hf_device_map.values()))  # Get the device of the first layer\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        # Tokenize batch and send it to the first layer's device\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, max_length = 4098, return_tensors=\"pt\").to(first_layer_device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract token embeddings\n",
    "        embeddings = last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)  # Normalize embeddings\n",
    "\n",
    "        # Collect embeddings on CPU\n",
    "        all_embeddings.append(embeddings)\n",
    "        print(f\"Encoded batch {i}\")\n",
    "    # Concatenate all embeddings\n",
    "    return torch.cat(all_embeddings, dim=0).cpu()\n",
    "\n",
    "def perform_inference(model, tokenizer, queries, corpus, top_k=25, output_path=\"./submission.parquet\", batch_size=64):\n",
    "    if not queries or not corpus:\n",
    "        raise ValueError(\"Queries or corpus cannot be empty.\")\n",
    "\n",
    "    # Step 1: Encode queries and corpus\n",
    "    print(\"Encoding queries...\")\n",
    "    query_embeddings = encode_sentences(list(queries.values()), model, tokenizer, batch_size=batch_size)\n",
    "\n",
    "    print(\"Encoding corpus...\")\n",
    "    corpus_embeddings = encode_sentences(list(corpus.values()), model, tokenizer, batch_size=batch_size)\n",
    "\n",
    "    # Step 2: Compute cosine similarity on CPU\n",
    "    print(\"Computing cosine similarity...\")\n",
    "    query_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "    corpus_norm = torch.nn.functional.normalize(corpus_embeddings, p=2, dim=1)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cos_sim_matrix = torch.mm(query_norm, corpus_norm.T)\n",
    "    #cos_sim_matrix = cosine_similarity(query_embeddings, corpus_embeddings.T)\n",
    "\n",
    "    # Step 3: Sort indices to get top-k results\n",
    "    print(\"Sorting results...\")\n",
    "    top_k_indices = torch.topk(cos_sim_matrix, k=top_k, dim=1).indices\n",
    "\n",
    "    # Step 4: Prepare output\n",
    "    output_data = []\n",
    "    query_keys = list(queries.keys())\n",
    "    corpus_keys = list(corpus.keys())\n",
    "\n",
    "    for i, query_id in enumerate(query_keys):\n",
    "        top_items = [corpus_keys[idx] for idx in top_k_indices[i].tolist()]\n",
    "        output_data.append({\"QuestionId_Answer\": query_id, \"MisconceptionId\": \" \".join(map(str, top_items))})\n",
    "\n",
    "    # Step 5: Save results to parquet\n",
    "    print(f\"Saving results to {output_path}...\")\n",
    "    result_df = pd.DataFrame(output_data)\n",
    "    result_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Inference results saved to {output_path}.\")\n",
    "\n",
    "perform_inference(model, tokenizer, queries, corpus, top_k=CONFIG[\"retrieve_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9be090b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:39:40.583204Z",
     "iopub.status.busy": "2024-12-11T13:39:40.582761Z",
     "iopub.status.idle": "2024-12-11T13:51:40.707449Z",
     "shell.execute_reply": "2024-12-11T13:51:40.706515Z"
    },
    "papermill": {
     "duration": 720.133981,
     "end_time": "2024-12-11T13:51:40.709423",
     "exception": false,
     "start_time": "2024-12-11T13:39:40.575442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result in function\r\n",
      "shape: (5, 10)\r\n",
      "┌─────────┬─────────┬─────────┬────────┬───┬────────┬────────┬────────┬────────┐\r\n",
      "│ Questio ┆ Constru ┆ Subject ┆ Questi ┆ … ┆ Answer ┆ AllTex ┆ Answer ┆ Questi │\r\n",
      "│ nId     ┆ ctName  ┆ Name    ┆ onText ┆   ┆ Text   ┆ t      ┆ Alphab ┆ onId_A │\r\n",
      "│ ---     ┆ ---     ┆ ---     ┆ ---    ┆   ┆ ---    ┆ ---    ┆ et     ┆ nswer  │\r\n",
      "│ i64     ┆ str     ┆ str     ┆ str    ┆   ┆ str    ┆ str    ┆ ---    ┆ ---    │\r\n",
      "│         ┆         ┆         ┆        ┆   ┆        ┆        ┆ str    ┆ str    │\r\n",
      "╞═════════╪═════════╪═════════╪════════╪═══╪════════╪════════╪════════╪════════╡\r\n",
      "│ 1869    ┆ Use the ┆ BIDMAS  ┆ \\[     ┆ … ┆ \\( 3   ┆ Constr ┆ A      ┆ 1869_A │\r\n",
      "│         ┆ order   ┆         ┆ 3      ┆   ┆ \\times ┆ uct    ┆        ┆        │\r\n",
      "│         ┆ of oper ┆         ┆ \\times ┆   ┆ (2+4)- ┆ Name:  ┆        ┆        │\r\n",
      "│         ┆ ations  ┆         ┆ 2+4-5  ┆   ┆ 5 \\)   ┆ Use    ┆        ┆        │\r\n",
      "│         ┆ to…     ┆         ┆ \\]     ┆   ┆        ┆ the    ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ Where  ┆   ┆        ┆ order  ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ do …   ┆   ┆        ┆ …      ┆        ┆        │\r\n",
      "│ 1869    ┆ Use the ┆ BIDMAS  ┆ \\[     ┆ … ┆ \\( 3   ┆ Constr ┆ B      ┆ 1869_B │\r\n",
      "│         ┆ order   ┆         ┆ 3      ┆   ┆ \\times ┆ uct    ┆        ┆        │\r\n",
      "│         ┆ of oper ┆         ┆ \\times ┆   ┆ 2+(4-5 ┆ Name:  ┆        ┆        │\r\n",
      "│         ┆ ations  ┆         ┆ 2+4-5  ┆   ┆ ) \\)   ┆ Use    ┆        ┆        │\r\n",
      "│         ┆ to…     ┆         ┆ \\]     ┆   ┆        ┆ the    ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ Where  ┆   ┆        ┆ order  ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ do …   ┆   ┆        ┆ …      ┆        ┆        │\r\n",
      "│ 1869    ┆ Use the ┆ BIDMAS  ┆ \\[     ┆ … ┆ \\( 3   ┆ Constr ┆ C      ┆ 1869_C │\r\n",
      "│         ┆ order   ┆         ┆ 3      ┆   ┆ \\times ┆ uct    ┆        ┆        │\r\n",
      "│         ┆ of oper ┆         ┆ \\times ┆   ┆ (2+4-5 ┆ Name:  ┆        ┆        │\r\n",
      "│         ┆ ations  ┆         ┆ 2+4-5  ┆   ┆ ) \\)   ┆ Use    ┆        ┆        │\r\n",
      "│         ┆ to…     ┆         ┆ \\]     ┆   ┆        ┆ the    ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ Where  ┆   ┆        ┆ order  ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ do …   ┆   ┆        ┆ …      ┆        ┆        │\r\n",
      "│ 1869    ┆ Use the ┆ BIDMAS  ┆ \\[     ┆ … ┆ Does   ┆ Constr ┆ D      ┆ 1869_D │\r\n",
      "│         ┆ order   ┆         ┆ 3      ┆   ┆ not    ┆ uct    ┆        ┆        │\r\n",
      "│         ┆ of oper ┆         ┆ \\times ┆   ┆ need   ┆ Name:  ┆        ┆        │\r\n",
      "│         ┆ ations  ┆         ┆ 2+4-5  ┆   ┆ bracke ┆ Use    ┆        ┆        │\r\n",
      "│         ┆ to…     ┆         ┆ \\]     ┆   ┆ ts     ┆ the    ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ Where  ┆   ┆        ┆ order  ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ do …   ┆   ┆        ┆ …      ┆        ┆        │\r\n",
      "│ 1870    ┆ Simplif ┆ Simplif ┆ Simpli ┆ … ┆ \\( m+1 ┆ Constr ┆ A      ┆ 1870_A │\r\n",
      "│         ┆ y an    ┆ ying    ┆ fy the ┆   ┆ \\)     ┆ uct    ┆        ┆        │\r\n",
      "│         ┆ algebra ┆ Algebra ┆ follow ┆   ┆        ┆ Name:  ┆        ┆        │\r\n",
      "│         ┆ ic frac ┆ ic Frac ┆ ing,   ┆   ┆        ┆ Simpli ┆        ┆        │\r\n",
      "│         ┆ tion…   ┆ tion…   ┆ if     ┆   ┆        ┆ fy an  ┆        ┆        │\r\n",
      "│         ┆         ┆         ┆ pos…   ┆   ┆        ┆ al…    ┆        ┆        │\r\n",
      "└─────────┴─────────┴─────────┴────────┴───┴────────┴────────┴────────┴────────┘\r\n",
      "--------\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [02:07<00:00, 42.38s/it]\r\n",
      "PEFT model loaded from '/kaggle/input/sfr-2-r-zeroshotselfconsistency-higherloradropout' across GPUs 1 and 2.\r\n",
      "Queries: 9\r\n",
      "Corpus: 2587\r\n",
      "Encoding queries...\r\n",
      "Encoded batch 0\r\n",
      "Encoding corpus...\r\n",
      "Encoded batch 0\r\n",
      "Encoded batch 64\r\n",
      "Encoded batch 128\r\n",
      "Encoded batch 192\r\n",
      "Encoded batch 256\r\n",
      "Encoded batch 320\r\n",
      "Encoded batch 384\r\n",
      "Encoded batch 448\r\n",
      "Encoded batch 512\r\n",
      "Encoded batch 576\r\n",
      "Encoded batch 640\r\n",
      "Encoded batch 704\r\n",
      "Encoded batch 768\r\n",
      "Encoded batch 832\r\n",
      "Encoded batch 896\r\n",
      "Encoded batch 960\r\n",
      "Encoded batch 1024\r\n",
      "Encoded batch 1088\r\n",
      "Encoded batch 1152\r\n",
      "Encoded batch 1216\r\n",
      "Encoded batch 1280\r\n",
      "Encoded batch 1344\r\n",
      "Encoded batch 1408\r\n",
      "Encoded batch 1472\r\n",
      "Encoded batch 1536\r\n",
      "Encoded batch 1600\r\n",
      "Encoded batch 1664\r\n",
      "Encoded batch 1728\r\n",
      "Encoded batch 1792\r\n",
      "Encoded batch 1856\r\n",
      "Encoded batch 1920\r\n",
      "Encoded batch 1984\r\n",
      "Encoded batch 2048\r\n",
      "Encoded batch 2112\r\n",
      "Encoded batch 2176\r\n",
      "Encoded batch 2240\r\n",
      "Encoded batch 2304\r\n",
      "Encoded batch 2368\r\n",
      "Encoded batch 2432\r\n",
      "Encoded batch 2496\r\n",
      "Encoded batch 2560\r\n",
      "Computing cosine similarity...\r\n",
      "Sorting results...\r\n",
      "Saving results to ./submission.parquet...\r\n",
      "Inference results saved to ./submission.parquet.\r\n"
     ]
    }
   ],
   "source": [
    "!python embedding_search_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6a87f",
   "metadata": {
    "papermill": {
     "duration": 0.008978,
     "end_time": "2024-12-11T13:51:40.727481",
     "exception": false,
     "start_time": "2024-12-11T13:51:40.718503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reranking Misconception\n",
    "Use Qwen2.5-14b-instruct-awq survivorship algorithm to rerank top-k misconception to the first k position\n",
    "Possible choices:\n",
    "1. Not using the algorithm at all (`selection_method = \"no_choose\"`)\n",
    "2. Use the algorithm (`selection_method = \"no_choose\"`), define the top k choice to be chosen in `top_k_choices` and the window for choice consideration as `m_window_option`. Aside from the requirements of `top_k_choices <= m_window_option`, note that the higher the top_k, the lower the window option, the better the result is, but will create more inference pass. To balance efficiency and performance boost, we use k=3, window_option=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84ce698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:51:40.747367Z",
     "iopub.status.busy": "2024-12-11T13:51:40.747028Z",
     "iopub.status.idle": "2024-12-11T13:51:40.756452Z",
     "shell.execute_reply": "2024-12-11T13:51:40.755574Z"
    },
    "papermill": {
     "duration": 0.021494,
     "end_time": "2024-12-11T13:51:40.758030",
     "exception": false,
     "start_time": "2024-12-11T13:51:40.736536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_output_selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_output_selection.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "\n",
    "#===== TUNE HERE ===========\n",
    "m_window_option = 15\n",
    "assert m_window_option <= 25\n",
    "top_k_choices = 3\n",
    "assert top_k_choices <= m_window_option\n",
    "#model_path = \"/flash2/aml/chenjiah24_wangwd24_lad24/Qwen2.5-7B-Instruct-AWQ\"\n",
    "selection_method = \"choose_k_from_all_with_mwindow\"\n",
    "assert selection_method in [\"no_choose\", \"choose_k_from_all_with_mwindow\"]\n",
    "model_path = \"/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ\"\n",
    "similarity_result_path = \"/kaggle/working/submission.parquet\"\n",
    "misconception_path = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\"\n",
    "reference_parquet_path = \"/kaggle/working/question_level_df.parquet\"\n",
    "#================================\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#convert pandas data into numpy for faster operation\n",
    "similarity_search_df = pd.read_parquet(similarity_result_path)\n",
    "#populate similarity_search_df with data from the train.csv\n",
    "reference_df = pd.read_parquet(reference_parquet_path)\n",
    "similarity_search_df = pd.merge(similarity_search_df, reference_df, on=['QuestionId_Answer'], how=\"left\")\n",
    "# Convert the 'MisconceptionId' column from a space-separated string to a list of integers\n",
    "similarity_search_df['MisconceptionId'] = similarity_search_df['MisconceptionId'].apply(lambda x: list(map(int, x.split())))\n",
    "\n",
    "# Convert the list of lists into a 2D NumPy array\n",
    "indices = np.array(similarity_search_df['MisconceptionId'].tolist())\n",
    "print(indices.shape)\n",
    "\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "# just directly give your answers.\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "\n",
    "misconception_df = pd.read_csv(misconception_path)\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "\n",
    "if selection_method == \"no_choose\":\n",
    "    #no need to do anything, assuming the similarity search already do its job\n",
    "    similarity_search_df = pd.read_parquet(similarity_result_path)\n",
    "    similarity_search_df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n",
    "elif selection_method in [\"choose_k_from_all_with_mwindow\"]:\n",
    "    def compute_rounds(T, M, C):\n",
    "        if M <= 0 or C <= 0:\n",
    "            raise ValueError(\"M and C must be greater than 0.\")\n",
    "        if T <= C:\n",
    "            return 0\n",
    "        R = (T - C + M - 1) // M  # ceiling division for (T-C)/M\n",
    "        return R\n",
    "\n",
    "    # Example usage\n",
    "    # Suppose indices is an N x T array of candidate IDs:\n",
    "    T = indices.shape[1] \n",
    "\n",
    "    M = m_window_option  # Candidates per round\n",
    "    C = top_k_choices   # Survivors chosen each round\n",
    "\n",
    "    R = compute_rounds(T, M, C)\n",
    "    print(\"Number of rounds (R):\", R)\n",
    "\n",
    "    # Initialize survivors with last C candidates\n",
    "    survivors = indices[:, -C:]\n",
    "    candidate_pool = indices[:, :-C]\n",
    "\n",
    "    for i in range(R):\n",
    "        # How many candidates this round?\n",
    "        pick = min(M, candidate_pool.shape[1])\n",
    "        \n",
    "        # Extract the last 'pick' candidates from candidate_pool\n",
    "        round_candidates = candidate_pool[:, -pick:]\n",
    "        candidate_pool = candidate_pool[:, :-pick] if candidate_pool.shape[1] > pick else candidate_pool[:, :0] #else empty the candidate pool\n",
    "        \n",
    "        # Combine with current survivors\n",
    "        c_indices = np.concatenate([round_candidates, survivors], axis=1)\n",
    "        \n",
    "        #empty the survivors\n",
    "        survivors = np.zeros((indices.shape[0], 0), dtype=int)\n",
    "\n",
    "        # Prepare for LLM inference\n",
    "        for infer_round in range(C):\n",
    "            similarity_search_df[f\"retrieval\"] = list(get_candidates(c_indices))  # Each row gets the array of candidates\n",
    "            similarity_search_df[\"text\"] = similarity_search_df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "\n",
    "        \n",
    "            #each inference can only do \n",
    "            responses = llm.generate(\n",
    "                similarity_search_df[\"text\"].values,\n",
    "                vllm.SamplingParams(\n",
    "                    n=1,\n",
    "                    top_k=1,\n",
    "                    temperature=0,\n",
    "                    seed=777,\n",
    "                    skip_special_tokens=False,\n",
    "                    max_tokens=1,\n",
    "                    logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[str(x) for x in range(1, c_indices.shape[1]+1)])]\n",
    "                ),\n",
    "                use_tqdm=True\n",
    "            )\n",
    "        \n",
    "            responses = [x.outputs[0].text for x in responses]\n",
    "            similarity_search_df[\"response\"] = responses\n",
    "            \n",
    "            llm_choices = similarity_search_df[\"response\"].astype(int).values - 1\n",
    "            \n",
    "            # Update survivors: pick the chosen candidate from each row\n",
    "            new_survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "\n",
    "            if infer_round < C - 1:\n",
    "                # Step 1: Create a mask to filter out the chosen indices\n",
    "                mask = np.ones(c_indices.shape, dtype=bool) # Start with a mask of all True values\n",
    "                np.put_along_axis(mask, llm_choices[:, np.newaxis], False, axis=1)  # Set the chosen indices to False\n",
    "\n",
    "                # Step 2: Apply the mask to filter out the chosen indices\n",
    "                c_indices = np.array([row[mask_row] for row, mask_row in zip(c_indices, mask)])\n",
    "\n",
    "            # Update survivors\n",
    "            survivors = np.concatenate([survivors, new_survivors], axis=1)\n",
    "\n",
    "    # After all rounds, 'survivors' holds the final chosen candidates\n",
    "    # Create final MisconceptionId column\n",
    "    results = []\n",
    "    for i in range(indices.shape[0]):\n",
    "        # survivors[i] is the chosen final candidate(s)\n",
    "        final_surv = survivors[i]\n",
    "        # All original candidates:\n",
    "        all_cands = indices[i]\n",
    "        chosen_set = list(final_surv) + [c for c in all_cands if c not in final_surv]\n",
    "        results.append(\" \".join(map(str, chosen_set)))\n",
    "\n",
    "    similarity_search_df[\"MisconceptionId\"] = results\n",
    "    similarity_search_df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)\n",
    "else:\n",
    "    raise ValueError(\"SelectionMethod Undefined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198e8696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:51:40.776574Z",
     "iopub.status.busy": "2024-12-11T13:51:40.776320Z",
     "iopub.status.idle": "2024-12-11T13:52:49.374368Z",
     "shell.execute_reply": "2024-12-11T13:52:49.373336Z"
    },
    "papermill": {
     "duration": 68.609754,
     "end_time": "2024-12-11T13:52:49.376427",
     "exception": false,
     "start_time": "2024-12-11T13:51:40.766673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 25)\r\n",
      "WARNING 12-11 13:51:45 config.py:246] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 12-11 13:51:45 config.py:715] Defaulting to use mp for distributed inference\r\n",
      "INFO 12-11 13:51:45 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ', speculative_config=None, tokenizer='/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=5120, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ, use_v2_block_manager=False, enable_prefix_caching=False)\r\n",
      "INFO 12-11 13:51:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "INFO 12-11 13:51:46 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:46 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 13:51:46 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:46 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:47 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:47 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "INFO 12-11 13:51:47 utils.py:784] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:47 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 13:51:47 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 12-11 13:51:48 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:48 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 12-11 13:51:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7cdc486ff250>, local_subscribe_port=51997, local_sync_port=55787, remote_subscribe_port=None, remote_sync_port=None)\r\n",
      "INFO 12-11 13:51:48 model_runner.py:680] Starting to load model /kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:48 model_runner.py:680] Starting to load model /kaggle/input/qwenqwen2-5-14b-instruct-awq/Qwen2.5-14B-Instruct-AWQ...\r\n",
      "INFO 12-11 13:51:48 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-11 13:51:48 selector.py:54] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:48 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:51:48 selector.py:54] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:08<00:16,  8.12s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:17<00:09,  9.09s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  7.75s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:24<00:00,  8.01s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=261)\u001b[0;0m INFO 12-11 13:52:12 model_runner.py:692] Loading model weights took 4.6719 GB\r\n",
      "INFO 12-11 13:52:12 model_runner.py:692] Loading model weights took 4.6719 GB\r\n",
      "INFO 12-11 13:52:17 distributed_gpu_executor.py:56] # GPU blocks: 4150, # CPU blocks: 2730\r\n",
      "Number of rounds (R): 2\r\n",
      "Processed prompts: 100%|█| 9/9 [00:04<00:00,  2.16it/s, est. speed input: 1053.8\r\n",
      "Processed prompts: 100%|█| 9/9 [00:04<00:00,  2.22it/s, est. speed input: 1040.7\r\n",
      "Processed prompts: 100%|█| 9/9 [00:04<00:00,  2.18it/s, est. speed input: 982.19\r\n",
      "Processed prompts: 100%|█| 9/9 [00:02<00:00,  3.12it/s, est. speed input: 1105.2\r\n",
      "Processed prompts: 100%|█| 9/9 [00:02<00:00,  3.46it/s, est. speed input: 1148.0\r\n",
      "Processed prompts: 100%|█| 9/9 [00:02<00:00,  3.71it/s, est. speed input: 1160.0\r\n",
      "INFO 12-11 13:52:45 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n",
      "Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stdout>'> at interpreter shutdown, possibly due to daemon threads\r\n",
      "Python runtime state: finalizing (tstate=0x00005bb6c4652a00)\r\n",
      "\r\n",
      "Current thread 0x00007cdd87cb8740 (most recent call first):\r\n",
      "  <no Python frame>\r\n",
      "\r\n",
      "Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, _cffi_backend, zstandard.backend_c, yaml._yaml, markupsafe._speedups, PIL._imaging, psutil._psutil_linux, psutil._psutil_posix, sentencepiece._sentencepiece, msgpack._cmsgpack, google.protobuf.pyext._message, google3.net.proto2.python.internal.cpp._message, setproctitle, uvloop.loop, ray._raylet, multidict._multidict, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, numexpr.interpreter, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, regex._regex, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._acero, pyarrow._csv, pyarrow._json, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, lz4._version, lz4.frame._frame, cython.cimports.libc.math, scipy._lib._ccallback_c, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.optimize._group_columns, scipy._lib.messagestream, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.spatial._ckdtree, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial.transform._rotation, scipy.optimize._direct, zmq.backend.cython._zmq (total: 160)\r\n"
     ]
    }
   ],
   "source": [
    "!python run_output_selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8b99e",
   "metadata": {
    "papermill": {
     "duration": 0.010282,
     "end_time": "2024-12-11T13:52:49.397449",
     "exception": false,
     "start_time": "2024-12-11T13:52:49.387167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b05896f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:52:49.420429Z",
     "iopub.status.busy": "2024-12-11T13:52:49.420076Z",
     "iopub.status.idle": "2024-12-11T13:52:49.434537Z",
     "shell.execute_reply": "2024-12-11T13:52:49.433725Z"
    },
    "papermill": {
     "duration": 0.027749,
     "end_time": "2024-12-11T13:52:49.436118",
     "exception": false,
     "start_time": "2024-12-11T13:52:49.408369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>2306 2488 2518 987 2221 1316 1999 2586 1672 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>2306 2488 2586 987 2221 1316 2518 1999 107 143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>2306 2488 1672 987 2221 1999 1316 2586 1119 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>891 2398 2142 1593 1540 1469 2021 2307 59 2549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>891 2398 59 1593 1540 2307 403 2021 1825 2549 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>891 2398 59 1593 1540 2307 403 1469 2021 1825 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1073 1287 2319 2439 632 188 691 2211 397 1873 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 397 2439 632 2211 691 188 2319 1200 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1287 1073 397 2439 632 2211 691 188 2319 1200 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  2306 2488 2518 987 2221 1316 1999 2586 1672 19...\n",
       "1            1869_C  2306 2488 2586 987 2221 1316 2518 1999 107 143...\n",
       "2            1869_D  2306 2488 1672 987 2221 1999 1316 2586 1119 25...\n",
       "3            1870_A  891 2398 2142 1593 1540 1469 2021 2307 59 2549...\n",
       "4            1870_B  891 2398 59 1593 1540 2307 403 2021 1825 2549 ...\n",
       "5            1870_C  891 2398 59 1593 1540 2307 403 1469 2021 1825 ...\n",
       "6            1871_A  1073 1287 2319 2439 632 188 691 2211 397 1873 ...\n",
       "7            1871_C  1287 1073 397 2439 632 2211 691 188 2319 1200 ...\n",
       "8            1871_D  1287 1073 397 2439 632 2211 691 188 2319 1200 ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"/kaggle/working/submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f428df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:52:49.458504Z",
     "iopub.status.busy": "2024-12-11T13:52:49.458182Z",
     "iopub.status.idle": "2024-12-11T13:52:49.465081Z",
     "shell.execute_reply": "2024-12-11T13:52:49.464234Z"
    },
    "papermill": {
     "duration": 0.02004,
     "end_time": "2024-12-11T13:52:49.466789",
     "exception": false,
     "start_time": "2024-12-11T13:52:49.446749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IS_SUBMISSION = False\\nif not IS_SUBMISSION:\\n    import pandas as pd\\n    from eedi_metrics import mapk, averaged_recall_at_k, averaged_ndcg_at_k\\n    # Read the label and predicted data\\n    label = pd.read_parquet(\"label.parquet\")\\n    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\\n    original_predicted_no_llm = pd.read_csv(\"/flash2/aml/lad24/submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\\n\\n    # Read question identifiers for both predictions\\n    question_ids = pd.read_csv(\"submission.csv\")[\"QuestionId_Answer\"]\\n    question_ids_no_llm = pd.read_csv(\"/flash2/aml/lad24/submission.csv\")[\"QuestionId_Answer\"]\\n\\n    # Filter and reorder labels for `predicted`\\n    filtered_label = label[label[\"QuestionId_Answer\"].isin(question_ids)]\\n    filtered_label = filtered_label.set_index(\"QuestionId_Answer\").loc[question_ids].reset_index()\\n\\n    # Filter and reorder labels for `original_predicted_no_llm`\\n    filtered_label_no_llm = label[label[\"QuestionId_Answer\"].isin(question_ids_no_llm)]\\n    filtered_label_no_llm = filtered_label_no_llm.set_index(\"QuestionId_Answer\").loc[question_ids_no_llm].reset_index()\\n\\n    # Calculate metrics\\n    print(\"Validation original MAP25: \", mapk(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\\n    print(\"Validation MAP25: \", mapk(filtered_label[\"MisconceptionId\"].tolist(), predicted))\\n    print(\"Validation original Recall25: \", averaged_recall_at_k(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\\n    print(\"Validation Recall2525: \", averaged_recall_at_k(filtered_label[\"MisconceptionId\"].tolist(), predicted))\\n    print(\"Validation original NDCG25: \", averaged_ndcg_at_k(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\\n    print(\"Validation NDCG25: \", averaged_ndcg_at_k(filtered_label[\"MisconceptionId\"].tolist(), predicted))'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"IS_SUBMISSION = False\n",
    "if not IS_SUBMISSION:\n",
    "    import pandas as pd\n",
    "    from eedi_metrics import mapk, averaged_recall_at_k, averaged_ndcg_at_k\n",
    "    # Read the label and predicted data\n",
    "    label = pd.read_parquet(\"label.parquet\")\n",
    "    predicted = pd.read_csv(\"submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n",
    "    original_predicted_no_llm = pd.read_csv(\"/flash2/aml/lad24/submission.csv\")[\"MisconceptionId\"].apply(lambda x: [int(y) for y in x.split()])\n",
    "\n",
    "    # Read question identifiers for both predictions\n",
    "    question_ids = pd.read_csv(\"submission.csv\")[\"QuestionId_Answer\"]\n",
    "    question_ids_no_llm = pd.read_csv(\"/flash2/aml/lad24/submission.csv\")[\"QuestionId_Answer\"]\n",
    "\n",
    "    # Filter and reorder labels for `predicted`\n",
    "    filtered_label = label[label[\"QuestionId_Answer\"].isin(question_ids)]\n",
    "    filtered_label = filtered_label.set_index(\"QuestionId_Answer\").loc[question_ids].reset_index()\n",
    "\n",
    "    # Filter and reorder labels for `original_predicted_no_llm`\n",
    "    filtered_label_no_llm = label[label[\"QuestionId_Answer\"].isin(question_ids_no_llm)]\n",
    "    filtered_label_no_llm = filtered_label_no_llm.set_index(\"QuestionId_Answer\").loc[question_ids_no_llm].reset_index()\n",
    "\n",
    "    # Calculate metrics\n",
    "    print(\"Validation original MAP25: \", mapk(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\n",
    "    print(\"Validation MAP25: \", mapk(filtered_label[\"MisconceptionId\"].tolist(), predicted))\n",
    "    print(\"Validation original Recall25: \", averaged_recall_at_k(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\n",
    "    print(\"Validation Recall2525: \", averaged_recall_at_k(filtered_label[\"MisconceptionId\"].tolist(), predicted))\n",
    "    print(\"Validation original NDCG25: \", averaged_ndcg_at_k(filtered_label_no_llm[\"MisconceptionId\"].tolist(), original_predicted_no_llm))\n",
    "    print(\"Validation NDCG25: \", averaged_ndcg_at_k(filtered_label[\"MisconceptionId\"].tolist(), predicted))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b3f6a",
   "metadata": {
    "papermill": {
     "duration": 0.010318,
     "end_time": "2024-12-11T13:52:49.487680",
     "exception": false,
     "start_time": "2024-12-11T13:52:49.477362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5007450,
     "sourceId": 8413131,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6193445,
     "sourceId": 10051863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256361,
     "sourceId": 10137125,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6265689,
     "sourceId": 10149662,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10160879,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1128.351147,
   "end_time": "2024-12-11T13:52:50.518740",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T13:34:02.167593",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
